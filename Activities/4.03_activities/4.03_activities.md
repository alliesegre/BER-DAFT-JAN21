4.03_solution_activity_1.md
pattern = '\d+\s*-\s*\d+\s*-\s*\d+'
re.findall(pattern, text)
pattern = '[A-Z][a-z]+ ?[A-Z][a-z]+|[A-Z][a-z]+'
print(re.findall(pattern, text))

4.03_solution_activity_2.md
domain_categories = {"U" : "Urban", "C" : "City", "S" : "Suburban", "T" : "Town", "R" : "Rural"}
def clean_domain(x):
    if x[0] in list(domain_categories.keys()):
        return domain_categories[x[0]]
    else:
        return np.NaN
data['DOMAIN'] = list(map(clean_domain, data["DOMAIN"]))
Replacing null values in the column with the most represented category
copy['DOMAIN'].isna().sum() # Check if there are any null values
copy['DOMAIN'].value_counts() # Check the most represented category

copy['DOMAIN'] = copy['DOMAIN'].fillna('Suburban')


4.03_solution_activity_3.md
data_corr = data[['INCOME', 'HV1', 'HV2', 'IC1', 'IC2', 'IC3', 'IC4', 'NUMPROM', 'CARDPROM', 'NGIFTALL', 'TIMELAG', 'AVGGIFT']]
corr_matrix=data_corr.corr(method='pearson')
fig, ax = plt.subplots(figsize=(10, 8))
ax = sns.heatmap(corr_matrix, annot=True)
plt.show()

sns.regplot('HV1','HV2', data=data_corr)
sns.regplot('HV1','HV3', data=data_corr)
sns.regplot('HV1','HV4', data=data_corr)
sns.regplot('IC1','IC2', data=data_corr)

model = LinearRegression().fit(data_corr[['HV1']], data_corr[['HV2']])
model.score(data_corr[['HV1']], data_corr[['HV2']])

model = LinearRegression().fit(data_corr[['IC1']], data_corr[['IC2']])
model.score(data_corr[['IC1']], data_corr[['IC2']])

4.03_solution_activity_4.md
There is not a correct answer, both are equally useful approaches to improve the quality of our data. In the case of multicollinearity, why not check both? If two or more columns are directly correlated for more than a 0.9 you may want to drop it, same for VIF over 10. Just check both and if one exceeds those numbers try to drop it.

# using corr

flag= True
while flag is True:
    flag = False
    for i in range(1, corr_matrix.shape[1]):
        if corr_matrix.iloc[i,range(i)].max() > 0.9:
            print(corr_matrix.columns[corr_matrix.iloc[i,range(i)].argmax()])
            col_name = corr_matrix.columns[corr_matrix.iloc[i,range(i)].argmax()]
            corr_matrix.drop([col_name], axis=1, inplace=True)
            corr_matrix.drop([col_name], inplace=True)

            flag = True
            break

print(corr_matrix.columns)

# using VIF
flag = True
threshold = 50
data_corr = add_constant(data_corr)
while flag is True:
    #print(data_corr.head())
    flag = False
    values = [variance_inflation_factor(np.array(data_corr), i) for i in np.arange(data_corr.shape[1])]
    #print(values)
    if max(values)> threshold:
        col_index = values.index(max(values))
        column_name = data_corr.columns[col_index]
        data_corr = data_corr.drop([column_name], axis=1)
        flag = True

print(data_corr.columns)