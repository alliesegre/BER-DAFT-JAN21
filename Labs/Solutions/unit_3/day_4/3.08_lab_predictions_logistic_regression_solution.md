```sql
-- 1 Create a query or queries to extract the information you think may be relevant
-- for building the prediction model. It should include some film features and some rental features.
select i.film_id, avg(p.amount) avg_rental_cost,
case when timestampdiff(hour, r.rental_date, r.return_date) regexp '^[0-9]+$'
then avg(timestampdiff(hour, r.rental_date, r.return_date))
else 0
end as avg_hours_rented, count(ifnull(r.rental_id, 0)) num_rent
from rental r
join payment p on p.rental_id = r.rental_id
join inventory i on i.inventory_id = r.inventory_id
group by film_id;

select
  f.film_id,
  f.title,
  f.description,
  fc.category_id,
  f.language_id,
  avg(f.rental_duration) * 24 avg_hours_rental_allowed,
  f.length / 60 hours_length,
  avg(f.replacement_cost) avg_replacement_cost,
  f.rating,
  f.special_features,
  count(fa.actor_id) actors_in_film
from film f
join film_category fc on fc.film_id = f.film_id
join film_actor fa on fa.film_id = f.film_id
group by f.film_id;

select
  act2.film_id,
  group_concat(act2.actor_id separator ',') actor_list,
  sum(act2.actor_fame) total_actor_fame,
  sum(act2.actor_influence) total_actor_influence
from (
  select fa.film_id, act1.*
    from (
      select
        fa1.actor_id,
        count(distinct(fa1.film_id)) actor_fame,
        count(distinct(fa2.actor_id)) actor_influence
        from
          film_actor fa1
          join film_actor fa2 on fa2.film_id = fa1.film_id
          group by fa1.actor_id
    ) act1
    join film_actor fa on fa.actor_id = act1.actor_id
) act2
group by act2.film_id;
```

```python
# Connect to the database
import pymysql
from sqlalchemy import create_engine
import pandas as pd
import getpass  # To get the password without showing the input

password = getpass.getpass()
db_url = f'mysql+pymysql://root:{password}@localhost/sakila'


#  Read the data into a Pandas dataframe.
query = '''
select i.film_id, avg(p.amount) avg_rental_cost,
case when timestampdiff(hour, r.rental_date, r.return_date) regexp '^[0-9]+$'
then avg(timestampdiff(hour, r.rental_date, r.return_date))
else 0
end as avg_hours_rented, count(ifnull(r.rental_id, 0)) num_rent
from rental r
join payment p on p.rental_id = r.rental_id
join inventory i on i.inventory_id = r.inventory_id
group by film_id;

select
  f.film_id,
  f.title,
  f.description,
  fc.category_id,
  f.language_id,
  avg(f.rental_duration) * 24 avg_hours_rental_allowed,
  f.length / 60 hours_length,
  avg(f.replacement_cost) avg_replacement_cost,
  f.rating,
  f.special_features,
  count(fa.actor_id) actors_in_film
from film f
join film_category fc on fc.film_id = f.film_id
join film_actor fa on fa.film_id = f.film_id
group by f.film_id;

select
  act2.film_id,
  group_concat(act2.actor_id separator ',') actor_list,
  sum(act2.actor_fame) total_actor_fame,
  sum(act2.actor_influence) total_actor_influence
from (
  select fa.film_id, act1.*
    from (
      select
        fa1.actor_id,
        count(distinct(fa1.film_id)) actor_fame,
        count(distinct(fa2.actor_id)) actor_influence
        from
          film_actor fa1
          join film_actor fa2 on fa2.film_id = fa1.film_id
          group by fa1.actor_id
    ) act1
    join film_actor fa on fa.actor_id = act1.actor_id
) act2
group by act2.film_id;'''

data = pd.read_sql(query, db_url)


## 3- Analyze extracted features and transform them.
# You may need to encode some categorical variables, or scale numerical variables.
X = data[['avg_rental_cost', 'avg_hours_rented', 'category_id', 'hours_length', 'actors_in_film', 'total_actor_fame']]

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

scaled_x = StandardScaler().fit_transform(X)


## read the target data into Python

target_query = '''
select rental_id, film_id, rental_date, rank() over (partition by film_id order by rental_date desc) recent
from film left join inventory_id using (film_id) left join rental using (inventory_id)
where rental_date > '2006-03-01'
'''

target = pd.read_sql(target_query, db_url)
y = target['rental_id'].isna()

## Create a logistic regression model to predict this variable from the cleaned data.
classification = LogisticRegression(random_state=0, solver='lbfgs',
                        multi_class='ovr').fit(scaled_x, y)

## Evaluate the results.
classification.score(scaled_x, y)

from sklearn.metrics import confusion_matrix
y_pred = classification.predict(scaled_x)
confusion_matrix(y_pred, y)
```