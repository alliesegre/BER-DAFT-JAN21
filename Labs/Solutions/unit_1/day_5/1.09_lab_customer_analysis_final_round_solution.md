### 01 - Problem (case study)

Data Description.

- **Unnamed:** Index
- **customer:** Customer ID
- **state:** US State
- **customer_lifetime_value:** CLV is the client economic value for a company during all their relationship
- **response:** Response to marketing calls (customer engagement)
- **coverage:** Customer coverage type
- **education:** Customer education level
- **effective_to_date:** Effective to date
- **employmentstatus:** Customer employment status
- **gender:** Customer gender
- **income:** Customer income
- **location_code:** Customer living zone
- **marital_status:** Customer marital status
- **monthly_premium_auto:** Monthly premium
- **months_since_last_claim:** Last customer claim
- **months_since_policy_inception:** Policy Inception
- **number_of_open_complaints:** Open claims
- **number_of_policies:** Number policies
- **policy_type:** Policy type
- **policy:** Policy
- **renew_offer_type:** Renew
- **sales_channel:** Sales channel (customer-company first contact)
- **total_claim_amount:** Claims amount
- **vehicle_class:** Vehicle class
- **vehicle_size:** Vehicle size
- **vehicle_type:** Vehicle type

**Goal.**  
Can we predict the amount claimed by a client?

### 02 - Getting Data

- Read `.csv` file

```python
import pandas as pd                                           # panel data, handling dataframes
pd.set_option('display.max_columns', None)
```

```python
data=pd.read_csv('../data/marketing-customer-analysis.csv')    # import csv file
data.head()                                                    # show first 5 rows
```

### 03 - Cleaning/Wrangling/EDA

- Change headers names.

```python
data.shape       # dataframe dimensions
```

```python
data.columns     # columns headers
```

```python
data.columns=[e.lower().replace(' ', '_') for e in data.columns]   # lower and replace
data.columns
```

- Deal with NaN values.

```python
data.info(memory_usage='deep')   # dataframe info
```

```python
data.isna().sum()     # missing values
```

```python
data=data.drop(columns=['unnamed:_0', 'vehicle_type', 'customer'])   # drop useless columns (no info or nan)
```

```python
data=data.dropna()   # drop rows with nan values
```

```python
for c in data.columns.tolist():         # know the unique values for each column
    print(c, len(data[c].unique()))
```

```python
data.shape
```

- Categorical Features.

**Effective To Date**

```python
print('Original dtype: {}\n'.format(data['effective_to_date'].dtype))   # object
data['effective_to_date']=pd.to_datetime(data['effective_to_date'])   # datetime
print('Meantime dtype: {}'.format(data['effective_to_date'].dtype))
```

```python
print('--')
print('Min date: {}'.format(data['effective_to_date'].min()))         # from January 1st..
print('Max date: {}'.format(data['effective_to_date'].max()))         # to February 28th
print('--')
```

```python
data['effective_to_date']=data['effective_to_date'].apply(lambda x: x.toordinal())   # you can change the type to ordinal.

print('New dtype: {}'.format(data['effective_to_date'].dtype))
```

**Values for each class in categorical features**

```python
cat_cols=[col for col in data.columns if (data[col].dtype==object)]     # categorical columns

print('Categorical Features:', len(cat_cols))
print('----------')
for c in cat_cols:
    print('Name: {}'.format(data[c].name))    # column name
    print('Type: {}'.format(data[c].dtype))   # column type
    print('Unique values: {}'.format(len(data[c].unique())))   # column unique values
    print(data[c].unique())
    print(((data[c].value_counts()/ sum(data[c].value_counts()))*100))   # percentage
    print('\n----------')
```

- Numerical Features.

```python
data.describe()     # stats
```

```python
num_cols=[c for c in data.columns if (data[c].dtype!='object') and (c!='Effective To Date')]   # numerical columns

```

- Exploration.

**Bar plot for each categorical variable.**

```python
import matplotlib.pyplot as plt                 # visualization library
%matplotlib inline

for c in cat_cols:
    plt.figure(figsize=(10,5))
    plt.bar(data[c].unique(), data[c].value_counts())
    plt.title(c)
    plt.show();
```

```python
import seaborn as sns                           # visualization library, extends plt
sns.set(style="white")                          # style
```

**Correlation**

```python
import numpy as np    # numerical python, algebra library


corr=data.corr()      # compute the correlation matrix


mask=np.triu(np.ones_like(corr, dtype=np.bool))     # generate a mask for the upper triangle

f, ax=plt.subplots(figsize=(11, 9))                 # set up the matplotlib figure

cmap=sns.diverging_palette(220, 10, as_cmap=True)   # generate a custom diverging colormap

sns.heatmap(corr, mask=mask, cmap=cmap,             # draw the heatmap with the mask and correct aspect ratio
            vmax=.3, center=0, square=True,
            linewidths=.5, cbar_kws={"shrink": .5});
```

**All variables**

```python
sns.pairplot(data[num_cols]);
```

**Bar plot for each numerical variable.**

```python
for c in num_cols:
    plt.figure(figsize=(10,5))
    plt.hist(data[c])
    plt.title(c)
    plt.show();
```

**Box plot for each numerical variable for know outliers of each feature.**

```python
for c in num_cols:
    plt.figure(figsize=(10,5))
    plt.boxplot(data[c])
    plt.title(c)
    plt.show();
```

**Show a plot of the total number of response.**

```python
sns.countplot('response', data=data)
plt.ylabel('Total number of Response')
plt.show();
```

**Show a plot of the response rate by sales channel.**

```python
plt.figure(figsize=(8,4))
sns.countplot('response', hue='sales_channel', data=data)
plt.ylabel('Response by Sales Channel')
plt.show();
```

**Show a plot of the response rate by total claim amount.**

```python
plt.figure(figsize=(12,6))
sns.boxplot(y='total_claim_amount' , x='response', data=data)
plt.ylabel('Response by Total Claim Amount')
plt.show();
```

**Show a plot of the response rate by income.**

```python
plt.figure(figsize=(12,6))
sns.boxplot(y='income' , x='response', data=data)
plt.ylabel('Response by Inncome')
plt.show();
```

### 04 - Processing Data

- Dealing with outliers

```python
# e.g. 3*IQR in a column

q1=np.percentile(data['customer_lifetime_value'], 25)   # percentile 25
q3=np.percentile(data['customer_lifetime_value'], 75)   # percentile 75

iqr=q3-q1  # IQR

upper=q3+3*iqr   # upper boundary
lower=q1-3*iqr   # lower boundary
```

```python
len(data[data['customer_lifetime_value']<lower])
```

```python
len(data[data['customer_lifetime_value']>upper])
```

- Normalization

**Min-Max Scaler**

```python
from sklearn.preprocessing import MinMaxScaler

data['effective_to_date']=MinMaxScaler().fit_transform(data['effective_to_date'].values.reshape(-1, 1))

data['effective_to_date'].head()
```

**Standardize**

```python
from sklearn.preprocessing import StandardScaler

num_cols
```

```python
for c in num_cols[:-1]:   # we'll normalize all less the target column
    data[c]=StandardScaler().fit_transform(data[c].values.reshape(-1, 1))
```

```python
data.head()
```

- **Encoding Categorical Data**

```python
one_hot_data=pd.get_dummies(data[cat_cols], drop_first=True)   # one hot encoding categorical variables

one_hot_data.head()
```

**Concat numerical and categorical DataFrames**

```python
data=pd.concat([data, one_hot_data], axis=1)   # concat dataframes
data.drop(columns=cat_cols, inplace=True)
data.head()
```

- Splitting into train set and test set

```python
# first, split X-y (learning-target data)
X=data.drop(columns=['total_claim_amount'])
y=data['total_claim_amount']

# checking shape
print(X.shape)
print(y.shape)
```

```python
# train_test_split
from sklearn.model_selection import train_test_split as tts
```

```python
# train-test-split (4 sets)

X_train, X_test, y_train, y_test=tts(X, y, test_size=0.2, random_state=42)  # random state fixed sample
```

### 05 - Modeling

We have now the data prepared for the regression problem.

**Linear Regression**

```python
from sklearn.linear_model import LinearRegression as LinReg
linreg=LinReg()    # model
linreg.fit(X_train, y_train)   # model train
y_pred_linreg=linreg.predict(X_test)   # model prediction
```

**Regularization**

```python
from sklearn.linear_model import Lasso       # L1
from sklearn.linear_model import Ridge       # L2
from sklearn.linear_model import ElasticNet  # L1+L2
```

```python
# Lasso L1

lasso=Lasso()
lasso.fit(X_train, y_train)

y_pred_lasso=lasso.predict(X_test)
```

```python
# Ridge L2

ridge=Ridge()
ridge.fit(X_train, y_train)

y_pred_ridge=ridge.predict(X_test)
```

```python
# ElasticNet L1+L2

elastic=ElasticNet()
elastic.fit(X_train, y_train)

y_pred_elastic=elastic.predict(X_test)
```

**Random Forest Regressor**

```python
from sklearn.ensemble import RandomForestRegressor as RFR

rfr=RFR()
rfr.fit(X_train, y_train)

y_pred_rfr=rfr.predict(X_test)
```

**XGBoost**

```python
from xgboost import XGBRegressor as XGBR

xgbr=XGBR()
xgbr.fit(X_train, y_train)

y_pred_xgbr=xgbr.predict(X_test)
```

**LightGBM**

```python
from lightgbm import LGBMRegressor as LGBMR

lgbmr=LGBMR()
lgbmr.fit(X_train, y_train)

y_pred_lgbmr=lgbmr.predict(X_test)
```

### 06 - Model Validation

```python
models=[linreg, lasso, ridge, elastic, rfr, xgbr, lgbmr]
model_names=['linreg', 'lasso', 'ridge', 'elastic', 'rfr', 'xgbr', 'lgbmr']
preds=[y_pred_linreg, y_pred_lasso, y_pred_ridge, y_pred_elastic, y_pred_rfr, y_pred_xgbr, y_pred_lgbmr]
```

- R2.

```python
for i in range(len(models)):

    train_score=models[i].score(X_train, y_train) #R2
    test_score=models[i].score(X_test, y_test)

    print ('Model: {}, train R2: {} -- test R2: {}'.format(model_names[i], train_score, test_score))
```

- MSE.

```python
from sklearn.metrics import mean_squared_error as mse

for i in range(len(models)):

    train_mse=mse(models[i].predict(X_train), y_train) #MSE
    test_mse=mse(preds[i], y_test)

    print ('Model: {}, train MSE: {} -- test MSE: {}'.format(model_names[i], train_mse, test_mse))
```

- **RMSE.**

```python
for i in range(len(models)):

    train_rmse=mse(models[i].predict(X_train), y_train)**0.5 #RMSE
    test_rmse=mse(preds[i], y_test)**0.5

    print ('Model: {}, train RMSE: {} -- test RMSE: {}'.format(model_names[i], train_rmse, test_rmse))
```

- MAE.

```python
from sklearn.metrics import mean_absolute_error as mae
for i in range(len(models)):
    train_mae=mae(models[i].predict(X_train), y_train) #MAE
    test_mae=mae(preds[i], y_test)

    print ('Model: {}, train MAE: {} -- test MAE: {}'.format(model_names[i], train_mae, test_mae))
```

### 07 - Reporting

- Present results.

**Data Level**

- Drop Nan values because they are, in fact, duplicates.
- Do not drop outliers because they are just a few.

**Problem Level**

- Total claim amount has a great variance.
- We can predict the total claim amount with a 25% of error, even when R2 is high.
- We need to determinate which are the significative variables.
